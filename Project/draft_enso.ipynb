{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bbfc9b-f32e-455c-b69e-3e08555cf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947a8a0a-dd78-4e87-965a-2b2e369a6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from navi import bearing_to_waypoint, rumbline_distance\n",
    "from utils import prepare_dataframe, generate_training_dataframe, cartesian_to_geo, get_direction_from_sin_cos, split_dataframe, manage_prediction, plot_tds, create_real_pred_df, probability_within_radius\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a4808-3a92-46cd-adbb-17ce47d4d455",
   "metadata": {},
   "source": [
    "# Draft Training and testing of models on predicting tropical depression, originating aa North pacific ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8150b68-9a36-4402-92f1-d6ce94c825aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc = pd.read_csv('data/csv_ready/ne_pacific_td.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a72d9c6-0187-4855-91ab-35682cb2d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4043e-f42a-4f18-990d-269070a21c69",
   "metadata": {},
   "source": [
    "In order to prepare the data for the model, we need to c/out data cleaning. Removing columns which are irelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f81a6f-bb05-441a-b05c-b0e04b57bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc = nhc.drop(columns=['basin', 'consecutive_count', 'type_of_depression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543a3df9-9eb6-4de6-8827-0fa87a191c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc.date = pd.to_datetime(nhc.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920c6cb9-866f-4bf0-8a70-c12f7a702b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "(nhc.date[1] - nhc.date[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5611d24c-ae32-45d1-8a8e-6b45a093f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_td.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e84fba-584a-4fab-9bf4-327277a63176",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c47efa-74ae-4041-8076-d75d1f2b47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = jma.drop(columns='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937f38e4-0fcc-4d96-98cc-59a24dfbab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.index = pd.to_datetime(jma.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22918be9-c3ee-41f3-9653-620417b4c0ae",
   "metadata": {},
   "source": [
    "Following will be c/out:\n",
    "1. drop columns\n",
    "2. if name is NaN, change to UNNAMED\n",
    "3. calculate velocity, direction of movement\n",
    "4. normalize(transform) direction into sin and cosin\n",
    "5. normalize velocity\n",
    "6. transform coordinates to cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fab9ac-bcef-4d29-8b1c-52ec5ea67c67",
   "metadata": {},
   "source": [
    "To transform geographic coordinates (latitude and longitude) into a trigonometric coordinate system, such as Cartesian coordinates (x, y, z), you can use a common spherical-to-Cartesian conversion. This transformation assumes the Earth as a sphere and projects the coordinates onto a 3D Cartesian system. Here’s how you can transform latitude and longitude into \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z coordinates:\n",
    "\n",
    "Transformation Formulas:\n",
    "Given:\n",
    "\n",
    "lat\n",
    "lat: Latitude in degrees\n",
    "lon\n",
    "lon: Longitude in degrees\n",
    "You can convert latitude and longitude into \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z using the following formulas:\n",
    "\n",
    "Convert latitude and longitude into radians:\n",
    "\n",
    "𝜙\n",
    "=\n",
    "lat\n",
    "×\n",
    "𝜋\n",
    "180\n",
    "ϕ=lat× \n",
    "180\n",
    "π\n",
    "​\n",
    " \n",
    "𝜆\n",
    "=\n",
    "lon\n",
    "×\n",
    "𝜋\n",
    "180\n",
    "λ=lon× \n",
    "180\n",
    "π\n",
    "​\n",
    " \n",
    "Where \n",
    "𝜙\n",
    "ϕ is latitude in radians and \n",
    "𝜆\n",
    "λ is longitude in radians.\n",
    "\n",
    "Calculate \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z assuming a unit sphere (Earth's radius is 1):\n",
    "\n",
    "𝑥\n",
    "=\n",
    "cos\n",
    "⁡\n",
    "(\n",
    "𝜙\n",
    ")\n",
    "×\n",
    "cos\n",
    "⁡\n",
    "(\n",
    "𝜆\n",
    ")\n",
    "x=cos(ϕ)×cos(λ)\n",
    "𝑦\n",
    "=\n",
    "cos\n",
    "⁡\n",
    "(\n",
    "𝜙\n",
    ")\n",
    "×\n",
    "sin\n",
    "⁡\n",
    "(\n",
    "𝜆\n",
    ")\n",
    "y=cos(ϕ)×sin(λ)\n",
    "𝑧\n",
    "=\n",
    "sin\n",
    "⁡\n",
    "(\n",
    "𝜙\n",
    ")\n",
    "z=sin(ϕ)\n",
    "\n",
    "Explanation:\n",
    "Latitude (lat): Measures the angle above or below the equator.\n",
    "Longitude (lon): Measures the angle east or west of the prime meridian.\n",
    "The trigonometric transformation projects these angles into a Cartesian 3D space using cosine and sine functions. This helps convert the Earth’s surface (which is spherical) into a Cartesian coordinate system.\n",
    "What the \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, \n",
    "𝑧\n",
    "z Coordinates Represent:\n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z represent the position on a unit sphere where the radius is 1.\n",
    "The benefit of using \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z instead of latitude and longitude is that the machine learning model can learn patterns more effectively in a continuous space rather than separate angular features.\n",
    "After this transformation, your ML model will work with \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z instead of lat/lon as separate features, preserving the spatial relationships in a more mathematically continuous way.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how about the reverse transformation?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To convert Cartesian coordinates (\n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, \n",
    "𝑧\n",
    "z) back to geographic coordinates (latitude and longitude), you can use the reverse spherical transformation. This involves converting the 3D Cartesian coordinates back into angles representing latitude and longitude.\n",
    "\n",
    "Reverse Transformation Formulas:\n",
    "Given Cartesian coordinates \n",
    "𝑥\n",
    "x, \n",
    "𝑦\n",
    "y, and \n",
    "𝑧\n",
    "z:\n",
    "\n",
    "Calculate the latitude (\n",
    "lat\n",
    "lat):\n",
    "\n",
    "lat\n",
    "=\n",
    "arcsin\n",
    "⁡\n",
    "(\n",
    "𝑧\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "𝑦\n",
    "2\n",
    "+\n",
    "𝑧\n",
    "2\n",
    ")\n",
    "lat=arcsin( \n",
    "x \n",
    "2\n",
    " +y \n",
    "2\n",
    " +z \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "z\n",
    "​\n",
    " )\n",
    "Since the unit sphere assumption means \n",
    "𝑥\n",
    "2\n",
    "+\n",
    "𝑦\n",
    "2\n",
    "+\n",
    "𝑧\n",
    "2\n",
    "=\n",
    "1\n",
    "x \n",
    "2\n",
    " +y \n",
    "2\n",
    " +z \n",
    "2\n",
    " \n",
    "​\n",
    " =1, this simplifies to:\n",
    "\n",
    "lat\n",
    "=\n",
    "arcsin\n",
    "⁡\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "lat=arcsin(z)\n",
    "Calculate the longitude (\n",
    "lon\n",
    "lon):\n",
    "\n",
    "lon\n",
    "=\n",
    "arctan\n",
    "⁡\n",
    "2\n",
    "(\n",
    "𝑦\n",
    ",\n",
    "𝑥\n",
    ")\n",
    "lon=arctan2(y,x)\n",
    "The arctan2 function is used to correctly compute the angle in all quadrants, considering both \n",
    "𝑥\n",
    "x and \n",
    "𝑦\n",
    "y values.\n",
    "\n",
    "Convert latitude and longitude from radians back to degrees:\n",
    "\n",
    "lat\n",
    "=\n",
    "lat\n",
    "×\n",
    "180\n",
    "𝜋\n",
    "lat=lat× \n",
    "π\n",
    "180\n",
    "​\n",
    " \n",
    "lon\n",
    "=\n",
    "lon\n",
    "×\n",
    "180\n",
    "𝜋\n",
    "lon=lon× \n",
    "π\n",
    "180\n",
    "​\n",
    "\n",
    "\n",
    "Explanation:\n",
    "Latitude is calculated using the inverse sine (\n",
    "arcsin\n",
    "⁡\n",
    "arcsin) of \n",
    "𝑧\n",
    "z, as the vertical component directly corresponds to latitude on a unit sphere.\n",
    "Longitude is derived using the \n",
    "arctan\n",
    "⁡\n",
    "2\n",
    "(\n",
    "𝑦\n",
    ",\n",
    "𝑥\n",
    ")\n",
    "arctan2(y,x), which correctly computes the angle in all quadrants. This gives you the correct longitude based on the \n",
    "𝑥\n",
    "x and \n",
    "𝑦\n",
    "y coordinates.\n",
    "Conversion to degrees is required because the \n",
    "arcsin\n",
    "⁡\n",
    "arcsin and \n",
    "arctan\n",
    "⁡\n",
    "2\n",
    "arctan2 functions return results in radians.\n",
    "Use Case:\n",
    "This reverse transformation is helpful if you want to transform your machine learning model's predictions (which could be in Cartesian coordinates) back into geographic latitude and longitude coordinates for easy interpretation and mapping.\n",
    "\n",
    "By combining both transformations (from geo to Cartesian and back), you ensure that geographic patterns in latitude and longitude are represented effectively for machine learning models, while still being able to interpret the results in traditional geographic terms.\n",
    "\n",
    "Feature engineering velocity and direction can be valuable, even though they are related to the coordinates. Here’s why they might still add value:\n",
    "\n",
    "1. Why Velocity and Direction Can Be Valuable:\n",
    "Velocity (speed of movement) and direction (bearing) capture the dynamics of the tropical depression (TD) over time. They provide insight into how fast and in what direction the TD is moving, which isn't always directly obvious from the raw coordinates.\n",
    "These features simplify complex relationships: Latitude and longitude alone are static points. Velocity and direction help capture temporal changes in location, which could be valuable for predicting future positions.\n",
    "Machine learning models sometimes benefit from derived features that highlight trends or movements explicitly, even if those features could technically be derived from other data (like coordinates).\n",
    "2. Velocity and Direction Calculations:\n",
    "Velocity measures the speed between two consecutive points (how fast the TD is moving).\n",
    "Direction (or bearing) measures the angle of movement between two consecutive points (in which direction the TD is moving).\n",
    "3. How They Add Value:\n",
    "Velocity is important in tracking how fast the tropical depression intensifies or weakens. Fast-moving TDs might behave differently than slow-moving ones.\n",
    "Direction is important because storms typically follow specific paths, and shifts in direction could indicate environmental changes (such as steering winds) that affect the storm's future track.\n",
    "4. Feature Redundancy Concern:\n",
    "While velocity and direction are derived from coordinates, they summarize movement in a way that’s often easier for the model to interpret than raw coordinate changes.\n",
    "If the model uses temporal information (like changes in coordinates over time), it might implicitly capture velocity and direction. However, explicitly adding them as features can make it easier for the model to learn these dynamics directly.\n",
    "5. Why They Might Not Add Value:\n",
    "If the model is already sophisticated enough to learn patterns from raw coordinates and time differences (e.g., in a sequence model like LSTM), then manually calculated velocity and direction might be redundant.\n",
    "In simpler models (like decision trees), these engineered features could be more valuable.\n",
    "Practical Approach:\n",
    "You could try both approaches:\n",
    "\n",
    "Train a model with just the coordinates and time.\n",
    "Train another model with coordinates, time, velocity, and direction as additional features.\n",
    "Then compare the performance of both models. If velocity and direction improve accuracy or interpretability, they are valuable. If not, they might be redundant in your specific case.\n",
    "\n",
    "6. How to Calculate Velocity and Direction:\n",
    "Velocity: The distance between two consecutive geographic points over a time period.\n",
    "Direction: The angle (bearing) between two consecutive geographic points.\n",
    "\n",
    "You're correct! Since direction (direction_deg) represents an angular value (e.g., compass direction), it behaves differently than continuous numerical features and should not be normalized the same way as features like velocity. Angular data has cyclic properties, where 0 degrees is equivalent to 360 degrees, which makes direct normalization problematic.\n",
    "\n",
    "How to handle direction_deg:\n",
    "Instead of normalizing it like a typical numerical feature, you can transform the direction into two separate features representing sine and cosine of the angle. This technique captures the cyclical nature of the data.\n",
    "\n",
    "Why this approach works:\n",
    "Sine and cosine represent the circular nature of angles, ensuring the model can interpret the closeness between directions like 0° and 360°.\n",
    "By splitting the angle into two components, the model can better understand the cyclical relationship between directions.\n",
    "Example:\n",
    "0° becomes (sin(0°), cos(0°)) = (0, 1)\n",
    "90° becomes (sin(90°), cos(90°)) = (1, 0)\n",
    "180° becomes (sin(180°), cos(180°)) = (0, -1)\n",
    "360° becomes (sin(360°), cos(360°)) = (0, 1) — same as 0°\n",
    "This transformation is essential when working with angular data to prevent issues that arise from the cyclic nature of angles, and it also ensures that the model treats directions correctly.\n",
    "\n",
    "Handling velocity:\n",
    "For velocity, you can still normalize it using either Min-Max Scaling or Z-Score Normalization as described earlier.\n",
    "\n",
    "This way, you ensure that direction is handled appropriately as a cyclical feature, and velocity is properly normalized as a continuous feature.\n",
    "\n",
    "To reverse the transformation from sine and cosine back to the direction in degrees, you can use the atan2 function, which computes the arctangent of the quotient of its arguments (in this case, the sine and cosine components). The atan2 function is helpful because it considers the sign of both inputs to determine the correct quadrant for the angle.\n",
    "\n",
    "Here’s how you can convert sine and cosine values back to the original direction (in degrees):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aece9458-886a-4a31-8e59-1f9242a98b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if name is NaN, change to UNNAMED\n",
    "jma['name'] = jma['name'].apply(lambda x: 'UNNAMED' if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1820b0-9cbf-4cff-95ab-b5331bf2fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = jma.drop(jma.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6be13b9-3c90-4995-b2fc-ab8eb2f8abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_to_cartesian(df):\n",
    "    \"\"\"\n",
    "    Transforms latitude and longitude into Cartesian coordinates (x, y, z).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame with 'lat' and 'lon' columns.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added 'x', 'y', 'z' columns.\n",
    "    \"\"\"\n",
    "    # Convert lat/lon from degrees to radians\n",
    "    lat_rad = np.radians(df['lat'])\n",
    "    lon_rad = np.radians(df['lon'])\n",
    "\n",
    "    # Compute x, y, z using spherical to Cartesian transformation\n",
    "    df['x'] = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "    df['y'] = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "    df['z'] = np.sin(lat_rad)\n",
    "\n",
    "    # df = df.drop(columns=['lat', 'lon'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def cartesian_to_geo(df):\n",
    "    \"\"\"\n",
    "    Transforms Cartesian coordinates (x, y, z) back to latitude and longitude.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame with 'x', 'y', 'z' columns.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added 'lat' and 'lon' columns.\n",
    "    \"\"\"\n",
    "    # Calculate latitude (in radians)\n",
    "    df['lat'] = np.degrees(np.arcsin(df['z']))\n",
    "    \n",
    "    # Calculate longitude (in radians), using arctan2 to handle all quadrants\n",
    "    df['lon'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "\n",
    "    df = df.drop(columns=['x', 'y', 'z'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_velocity_direction(df):\n",
    "    \"\"\"\n",
    "    Calculates velocity (kn) and direction (bearing in degrees) between consecutive points.\n",
    "    \"\"\"\n",
    "    velocity = np.zeros(len(df))\n",
    "    direction = np.zeros(len(df))\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # velocity and direction for the first record\n",
    "        if i == 0:\n",
    "            velocity[i] = 0\n",
    "            direction[i] = 0\n",
    "            continue\n",
    "\n",
    "        # check previous name is different from current name\n",
    "        previous_name = df.iloc[i-1]['name']\n",
    "        current_name = df.iloc[i]['name']\n",
    "        \n",
    "        if current_name != previous_name:\n",
    "            velocity[i] = 0\n",
    "            direction[i] = 0\n",
    "            continue\n",
    "        \n",
    "        # Get the previous and current coordinates\n",
    "        prev_point = (df.iloc[i-1]['lat'], df.iloc[i-1]['lon'])\n",
    "        curr_point = (df.iloc[i]['lat'], df.iloc[i]['lon'])\n",
    "        \n",
    "        # Calculate distance in nautical miles\n",
    "        distance = rumbline_distance(prev_point, curr_point)\n",
    "        \n",
    "        # Calculate time difference in hours\n",
    "        time_diff = (df.index[i] - df.index[i-1]).seconds / 3600\n",
    "        \n",
    "        # Calculate velocity in kn\n",
    "        if distance == 0 or time_diff ==0:\n",
    "            velocity[i] = 0\n",
    "        else:\n",
    "            velocity[i] = distance / time_diff\n",
    "        \n",
    "        # Calculate direction (bearing)\n",
    "        direction[i] = bearing_to_waypoint(prev_point, curr_point)\n",
    "    \n",
    "        \n",
    "    df.loc[:, 'velocity_kn'] = velocity\n",
    "    df.loc[:, 'direction_deg'] = direction\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_direction_to_sin_cosin(df):\n",
    "    direction_rad = np.deg2rad(df['direction_deg'])\n",
    "\n",
    "    # Create sine and cosine components\n",
    "    df['direction_sin'] = np.sin(direction_rad)\n",
    "    df['direction_cos'] = np.cos(direction_rad)\n",
    "    \n",
    "    # Drop the original direction column if it's no longer needed\n",
    "    # df = df.drop(columns=['direction_deg'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_direction_from_sin_cos(sin_val, cos_val):\n",
    "    \"\"\"\n",
    "    Reversed bearing in degrees from sin and cosin values.\n",
    "\n",
    "    Usage:\n",
    "    df['direction_deg_reversed'] = df.apply(lambda row: get_direction_from_sin_cos(row['direction_sin'], row['direction_cos']), axis=1)\n",
    "    \"\"\"\n",
    "    # Get the angle in radians\n",
    "    angle_rad = np.arctan2(sin_val, cos_val)\n",
    "    \n",
    "    # Convert radians to degrees\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    \n",
    "    # Make sure the angle is within the range 0° to 360°\n",
    "    if angle_deg < 0:\n",
    "        angle_deg += 360\n",
    "\n",
    "    return angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cfc057-7ac2-45fa-b50d-3150493db9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate velocity and direction\n",
    "jma = calculate_velocity_direction(jma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b0beaa-d081-4398-84f3-c9676f19e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize direction\n",
    "jma = convert_direction_to_sin_cosin(jma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3df5cc7c-b5c0-4169-9337-84e29b974a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = geo_to_cartesian(jma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27bef2bc-d9d4-42b8-8892-52f9283d6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize velocity\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "jma['velocity_kn'] = scaler.fit_transform(jma[['velocity_kn']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6a9f3b-becb-4b4d-9067-09da261d7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46470abf-59d2-47d4-9243-cac07013c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.groupby(jma.min_pressure_mBar).max_wind_kn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b93b6d1a-f340-41bb-89ec-f3ba7d50845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.groupby(jma.max_wind_kn).min_pressure_mBar.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ed7ca53-38c2-4ca9-89a2-1aaaf526d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a condition to set max_wind_kn to 35 where pressure is greater than or equal to 980\n",
    "jma.loc[jma['min_pressure_mBar'] >= 980, 'max_wind_kn'] = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74247ce5-e10f-4756-8a44-32daf58c92fd",
   "metadata": {},
   "source": [
    "Basis above separation, we will complete the missing data. As seen on the DS, in the first stages of TD, the wind record is 0. We will set that to 35 kn for concistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2aa3c49-543a-4506-94a2-f5659abcf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma['min_pressure_mBar'] = scaler.fit_transform(jma[['min_pressure_mBar']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f73dfa7-8c7d-4d2e-ae1d-12dbd0d8ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.max_wind_kn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b587d5da-afad-4032-9232-7a0e3c305454",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63ac64-d3f2-4f60-a010-cd6afc5a8ee6",
   "metadata": {},
   "source": [
    "Now to split the data and to train a model to be able to fill the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc8af70e-c541-4fc4-903d-6f2a659c6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with non-zero wind speed and prepare feature matrix and labels\n",
    "train_data = jma[jma['max_wind_kn'] > 0]\n",
    "X_train = train_data[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z']]\n",
    "y_train = train_data['max_wind_kn']\n",
    "\n",
    "# Filter rows with zero wind speed to predict later\n",
    "predict_data = jma[jma['max_wind_kn'] == 0]\n",
    "X_predict = predict_data[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z']]\n",
    "\n",
    "\n",
    "# Split training data into train and test sets for evaluation\n",
    "X_train_part, X_test_part, y_train_part, y_test_part = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = model.predict(X_test_part)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test_part, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1a744cb-b047-4d52-b963-cae9844d00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test_part, y_test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fb11233-54b0-4f9f-9206-81d0e60bfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict the wind speeds for the rows where wind is currently 0\n",
    "predicted_winds = model.predict(X_predict)\n",
    "\n",
    "# Update the original dataframe with the predicted values\n",
    "# If you want to cast to integers\n",
    "jma.loc[jma['max_wind_kn'] == 0, 'max_wind_kn'] = predicted_winds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "369e15f1-c702-4702-bad0-a72a623e5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "502d538a-7da6-4c26-8fae-d45d9d2fbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.max_wind_kn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22158bbd-5177-48a9-ba09-bc830dfab916",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "263e4988-f0ea-489d-8d6d-3478534eefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68026b2d-793b-4e6d-9b77-da6a77f6dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df5fe31b-0afa-4bf6-91e9-77c59fcaa232",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1b68b-8927-455f-887e-905a37dc716e",
   "metadata": {},
   "source": [
    "Generating pipeline\n",
    "Following will be c/out:\n",
    "\n",
    "read data\n",
    "read enso data\n",
    "index to datetime\n",
    "drop columns\n",
    "add enso\n",
    "if name is NaN, change to UNNAMED\n",
    "calculate velocity, direction of movement\n",
    "normalize(transform) direction into sin and cosin\n",
    "normalize velocity\n",
    "transform coordinates to cartesian\n",
    "fill 0 wind with min wind 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a625d2f-4e21-4507-8942-e1b8e41767e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "\n",
    "    def import_enso_to_df(row, enso_df):\n",
    "        \"\"\"\n",
    "        Usage: df['enso'] = df.apply(import_enso_to_df, axis=1, enso_df=enso_df)\n",
    "        \"\"\"\n",
    "        year = row.name.year\n",
    "        if year == 1949:\n",
    "            return -1\n",
    "        return enso_df.loc[enso_df['year'] == year].enso.values[0]\n",
    "    \n",
    "    # transform index column to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # read enso data and add the feature to df\n",
    "    enso_df = pd.read_csv('data/csv_ready/enso_years.csv')\n",
    "    df['enso'] = df.apply(import_enso_to_df, axis=1, enso_df=enso_df)\n",
    "\n",
    "    # fix the NaN of the TDs name\n",
    "    df['name'] = df['name'].apply(lambda x: 'UNNAMED' if pd.isna(x) else x)\n",
    "\n",
    "    df = df[['name', 'lat', 'lon', 'max_wind_kn', 'min_pressure_mBar', 'enso']]\n",
    "    \n",
    "    df = calculate_velocity_direction(df)  # Adds 'velocity_kn' and 'direction_deg'\n",
    "    df = convert_direction_to_sin_cosin(df)  # Converts 'direction_deg' to 'direction_sin' and 'direction_cos'\n",
    "    df = geo_to_cartesian(df)  # Converts lat/lon to 'x', 'y', and 'z' coordinates\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02aeddf7-659b-4a1e-bc21-02dd61b66154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jma = calculate_velocity_direction(jma)  # Adds 'velocity_kn' and 'direction_deg'\n",
    "# jma = convert_direction_to_sin_cosin(jma)  # Converts 'direction_deg' to 'direction_sin' and 'direction_cos'\n",
    "# jma = geo_to_cartesian(jma)  # Converts lat/lon to 'x', 'y', and 'z' coordinates\n",
    "\n",
    "# read jma data\n",
    "jma = pd.read_csv('data/csv_ready/jma_td.csv', index_col=0)\n",
    "# drop first row\n",
    "jma = jma.drop(jma.index[0])\n",
    "# add values for the wind\n",
    "jma.loc[jma['min_pressure_mBar'] >= 980, 'max_wind_kn'] = 35\n",
    "\n",
    "jma = prepare_dataframe(jma)\n",
    "\n",
    "available_data = jma[jma['max_wind_kn'] > 0]\n",
    "predicted_data = jma[jma['max_wind_kn'] == 0]\n",
    "\n",
    "X_raw = available_data[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]\n",
    "y_raw = available_data['max_wind_kn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n",
    "X_pred = predicted_data[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eeeb0438-37b0-4936-86b0-86b883fc2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('enso', OneHotEncoder(), ['enso']),\n",
    "    ('scaler', MinMaxScaler(), ['min_pressure_mBar', 'velocity_kn']),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),  # Apply scaling and encoding\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))  # Final model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d968586-a9e5-454c-926c-0338de6d3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce4d1e05-1779-4fa6-ae37-ad5c88e6819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring at test data is excellent\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c73d20f-1557-4e97-84c7-f3fa691f5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_winds = pipeline.predict(X_pred)\n",
    "\n",
    "# adding missing wind values to the dataframe\n",
    "jma.loc[jma['max_wind_kn'] == 0, 'max_wind_kn'] = predicted_winds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be30de02-8974-4381-b9c6-1f2117bd1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09136164-c3fd-4473-a9f5-980adbe149c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.max_wind_kn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08393b91-ab67-4549-b4f5-e3e8c2986b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "feature_names = pipeline.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4fe8353-f78f-4609-8184-60355b483ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44cb57b0-a4a6-408a-abf8-5393b91820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has missing pressure values. Will train the model to fill the missing values\n",
    "nhc.min_pressure_mBar.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7c060f8-60b0-4463-a87b-5b32d0a6b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc.index = pd.to_datetime(nhc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c35a765b-9816-4c40-9b0d-f83566e1f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac06a96b-8eed-4ca9-8316-debad3fc69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc = pd.read_csv('data/csv_ready/ne_pacific_td.csv', index_col=0)\n",
    "nhc = prepare_dataframe(nhc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31d7089c-f60b-4d94-97dd-b6a3c4434c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_data = nhc[nhc['min_pressure_mBar'] > 0]\n",
    "predicted_data = nhc[nhc['min_pressure_mBar'] < 0]\n",
    "\n",
    "X_raw = available_data[['max_wind_kn', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]\n",
    "y_raw = available_data['min_pressure_mBar']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n",
    "X_pred = predicted_data[['max_wind_kn', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1eb5116f-9bc0-4f3d-98fd-ae241c76a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_nhc = ColumnTransformer([\n",
    "    ('enso', OneHotEncoder(), ['enso']),\n",
    "    ('scaler', MinMaxScaler(), ['max_wind_kn', 'velocity_kn']),\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "367b392e-47e1-49d4-aa01-ab546b998e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nhc = Pipeline([\n",
    "    ('preprocess', preprocessor_nhc),  # Apply scaling and encoding\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d13fc106-1b66-4d54-8b52-ac10180b8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nhc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d4aa06b-8555-4d65-a886-a1de8090efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nhc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "356bc523-e636-4d2c-9daf-86261c97cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and evaluate\n",
    "y_pred = pipeline_nhc.predict(X_test)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27b6f37d-452d-43a8-a1b5-628259bad976",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipeline_nhc.named_steps['regressor'].feature_importances_\n",
    "feature_names = pipeline_nhc.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5dc711f-e25d-47bf-8333-3a510dcda8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pressure = pipeline_nhc.predict(X_pred)\n",
    "\n",
    "# adding missing wind values to the dataframe\n",
    "nhc.loc[nhc['min_pressure_mBar'] < 0, 'min_pressure_mBar'] = predicted_pressure.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8edeacc9-ca22-44db-842e-f53e99e7a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred.shape, predicted_pressure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65fed6c5-08d8-4448-afcf-20f125edcdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcc678f0-d323-46ac-b9f9-32adf9c11c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6820e6be-e3da-444c-963d-49f1a587bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jma.to_csv('data/csv_ready/jma_final')\n",
    "# nhc.to_csv('data/csv_ready/nhc_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5757fe36-7257-48cd-b0f1-d6aba8f5f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need unique identifier for the tds\n",
    "jma = pd.read_csv('data/csv_ready/jma_final', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2f800ab-bcb5-487e-bb74-71f7cc2c019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5521d567-8b26-4e65-9d2f-9c8755b38813",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3102d-eaec-41bd-9a69-20a1d8e04347",
   "metadata": {},
   "source": [
    "To train a model that predicts the tropical depression's next coordinates (latitude and longitude), direction, velocity, pressure, and windspeed while considering the ENSO phase, you need to set up a multivariate regression model. Here's how you can approach this:\n",
    "\n",
    "Approach Overview:\n",
    "Input Features:\n",
    "\n",
    "Latitude (lat)\n",
    "Longitude (lon)\n",
    "Wind speed (max_wind_kn)\n",
    "Minimum central pressure (min_pressure_mBar)\n",
    "Velocity (velocity_kn)\n",
    "Direction (sine and cosine components of the direction, direction_sin and direction_cos)\n",
    "ENSO Phase (One-hot encoded)\n",
    "Target Variables (Outputs):\n",
    "\n",
    "Next latitude (lat)\n",
    "Next longitude (lon)\n",
    "Next wind speed (max_wind_kn)\n",
    "Next pressure (min_pressure_mBar)\n",
    "Next velocity (velocity_kn)\n",
    "Next direction (sine and cosine components)\n",
    "Feature Engineering:\n",
    "\n",
    "Create lag features from the current observation to the next (e.g., current lat/lon to predict next lat/lon).\n",
    "Use the shift() function to generate target values for the next time step.\n",
    "Model Setup: You can use a multi-output regressor that predicts multiple target variables simultaneously, such as Random Forest, Gradient Boosting, or XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d844442-cf71-4b08-a588-2c8eb963dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create shifted target variables\n",
    "# jma['next_x'] = jma['x'].shift(-1)\n",
    "# jma['next_y'] = jma['y'].shift(-1)\n",
    "# jma['next_z'] = jma['z'].shift(-1)\n",
    "# jma['next_max_wind_kn'] = jma['max_wind_kn'].shift(-1)\n",
    "# jma['next_min_pressure_mBar'] = jma['min_pressure_mBar'].shift(-1)\n",
    "# jma['next_velocity_kn'] = jma['velocity_kn'].shift(-1)\n",
    "# jma['next_direction_sin'] = jma['direction_sin'].shift(-1)\n",
    "# jma['next_direction_cos'] = jma['direction_cos'].shift(-1)\n",
    "\n",
    "# # Drop rows with NaN in shifted columns (this will occur at the last row due to the shift)\n",
    "# jma.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "221a372b-29aa-4231-8c0c-ae9a9d94513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepering the data for the model\n",
    "# X = jma[['min_pressure_mBar', 'enso', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z']]  # all features except the target\n",
    "# y = jma['max_wind_kn']  # target variable (wind speed)\n",
    "\n",
    "# groups = jma['count']\n",
    "\n",
    "# # Perform GroupShuffleSplit to keep all observations of each TD together\n",
    "# gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=97)\n",
    "# train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "# # Step 5: Split the dataset\n",
    "# X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "# y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d0af3b6-24de-4f24-89bd-4071d839e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_group(df_group):\n",
    "    \"\"\"\n",
    "    Shift features to create target variables for the next observation within each group (TD).\n",
    "    jma = jma.groupby('id').apply(shift_group).reset_index(drop=True)\n",
    "    \"\"\"\n",
    "    df_group['next_x'] = df_group['x'].shift(-1)\n",
    "    df_group['next_y'] = df_group['y'].shift(-1)\n",
    "    df_group['next_z'] = df_group['z'].shift(-1)\n",
    "    df_group['next_max_wind_kn'] = df_group['max_wind_kn'].shift(-1)\n",
    "    df_group['next_min_pressure_mBar'] = df_group['min_pressure_mBar'].shift(-1)\n",
    "    df_group['next_velocity_kn'] = df_group['velocity_kn'].shift(-1)\n",
    "    df_group['next_direction_sin'] = df_group['direction_sin'].shift(-1)\n",
    "    df_group['next_direction_cos'] = df_group['direction_cos'].shift(-1)\n",
    "    \n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf08da4e-c4cd-49cd-9126-0d98c8fd3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = jma.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3519c37-1998-4ec4-99f2-7cbb190de3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply shift within each TD group\n",
    "jma = jma[:10].groupby('id').apply(shift_group).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dea8c38c-932b-4807-a70c-862112f413b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.index = jma.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12935193-3561-4485-8640-eb1e0b14144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42b4058d-50f2-4f9a-9fa8-9268a9cc0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_final', index_col=0)\n",
    "\n",
    "# Apply shift within each TD group\n",
    "jma = jma.groupby('id').apply(shift_group, include_groups=False)\n",
    "\n",
    "jma = jma.reset_index()\n",
    "\n",
    "jma.index = pd.to_datetime(jma.date)\n",
    "\n",
    "jma = jma.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5dc48ddd-3a0e-4e21-8238-0c4a3707897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Drop rows where targets are NaN (this happens for the last observation in each group)\n",
    "jma = jma.dropna()\n",
    "\n",
    "# Features and target columns\n",
    "X = jma[['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', 'velocity_kn', \n",
    "        'direction_sin', 'direction_cos', 'enso']]\n",
    "\n",
    "y = jma[['next_x', 'next_y', 'next_z', 'next_max_wind_kn', 'next_min_pressure_mBar', \n",
    "        'next_velocity_kn', 'next_direction_sin', 'next_direction_cos']]\n",
    "\n",
    "# Step 3: Use GroupKFold to split while maintaining TD groups\n",
    "groups = jma['id']\n",
    "\n",
    "# Define GroupKFold object (5 splits in this example)\n",
    "gkf = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f0fc801-3d0c-46e2-8ff4-0a25e10777c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train/Test Split by Groups\n",
    "%timeit\n",
    "for train_index, test_index in gkf.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Step 5: Define the model pipeline\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "        ('scaler', MinMaxScaler(), ['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', \n",
    "                                    'velocity_kn', 'direction_sin', 'direction_cos'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=97)))\n",
    "    ])\n",
    "\n",
    "    # Step 6: Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Step 7: Predict and evaluate\n",
    "    score = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb11bf2-f18c-4fa9-a575-7d9d97ac7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48370785-a5e0-44e4-8c00-5123157d1e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abe134-f51f-48e2-947b-55d1554dace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = model['regressor'].estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e3dbe-6782-48d8-a530-37678a72cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = random.named_steps['regressor'].feature_importances_\n",
    "feature_names = random.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ef463-dd81-4b2e-9c1a-9b4b4b070532",
   "metadata": {},
   "outputs": [],
   "source": [
    "model['regressor'].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6cfe1-cd51-473d-ba29-c90ad784784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jma[['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', 'velocity_kn', \n",
    "        'direction_sin', 'direction_cos', 'enso']]\n",
    "\n",
    "y = jma[['next_x', 'next_y', 'next_z', 'next_max_wind_kn', 'next_min_pressure_mBar', \n",
    "        'next_velocity_kn', 'next_direction_sin', 'next_direction_cos']]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=97)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "# Step 5: Split the dataset\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58f20a-dd69-4ecf-a129-f9b3fe971c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "        ('scaler', MinMaxScaler(), ['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', \n",
    "                                    'velocity_kn', 'direction_sin', 'direction_cos'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=97)))\n",
    "    ])\n",
    "\n",
    "# Step 6: Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict and evaluate\n",
    "score = model.score(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8564fd-ffa0-44f7-bf99-2d073dda3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc = pd.read_csv('data/csv_ready/nhc_final', index_col=0)\n",
    "\n",
    "# Apply shift within each TD group\n",
    "nhc = nhc.groupby('id').apply(shift_group, include_groups=False)\n",
    "\n",
    "nhc = nhc.reset_index()\n",
    "\n",
    "nhc.index = pd.to_datetime(nhc.date)\n",
    "\n",
    "nhc = nhc.drop(columns=['date'])\n",
    "nhc = nhc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04ad5b-1302-49bc-8a32-6a41d8c2d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_grouped_dataframe(df, splitter='gkf', n_splits=1, test_size=0.2, random_state=97):\n",
    "    \"\"\"\n",
    "    Splits the grouped dataframe based on the given splitter (GroupKFold or GroupShuffleSplit).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the time series data.\n",
    "    splitter (str): Type of splitter to use, either 'gkf' (GroupKFold) or 'gss' (GroupShuffleSplit).\n",
    "    n_splits (int): Number of splits for GroupKFold (defaults to 1).\n",
    "    test_size (float): Proportion of data to use as the test set for GroupShuffleSplit.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Yields:\n",
    "    X_train, X_test, y_train, y_test: Split training and testing sets for features and targets.\n",
    "\n",
    "    Example of using the function to get multiple train-test splits\n",
    "    for X_train, X_test, y_train, y_test in split_grouped_dataframe(df, splitter='gkf', n_splits=5):\n",
    "        print(\"Training split shape:\", X_train.shape)\n",
    "        print(\"Test split shape:\", X_test.shape)\n",
    "        \n",
    "    # Add model training and evaluation here\n",
    "    \"\"\"\n",
    "\n",
    "    def shift_group(df_group):\n",
    "        \"\"\"\n",
    "        Shift features to create target variables for the next observation within each group (TD).\n",
    "        \"\"\"\n",
    "        df_group['next_x'] = df_group['x'].shift(-1)\n",
    "        df_group['next_y'] = df_group['y'].shift(-1)\n",
    "        df_group['next_z'] = df_group['z'].shift(-1)\n",
    "        df_group['next_max_wind_kn'] = df_group['max_wind_kn'].shift(-1)\n",
    "        df_group['next_min_pressure_mBar'] = df_group['min_pressure_mBar'].shift(-1)\n",
    "        df_group['next_velocity_kn'] = df_group['velocity_kn'].shift(-1)\n",
    "        df_group['next_direction_sin'] = df_group['direction_sin'].shift(-1)\n",
    "        df_group['next_direction_cos'] = df_group['direction_cos'].shift(-1)\n",
    "        return df_group\n",
    "\n",
    "    # Shift features within each group to create targets\n",
    "    df = df.groupby('id').apply(shift_group, include_groups=False)\n",
    "\n",
    "    # Reset index and drop unnecessary columns\n",
    "    df = df.reset_index()\n",
    "    df.index = pd.to_datetime(df['date'])\n",
    "    df = df.drop(columns=['date'])\n",
    "\n",
    "    # drop NaN containing lines\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Define the features and target columns\n",
    "    X = df[['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', 'velocity_kn', \n",
    "            'direction_sin', 'direction_cos', 'enso']]\n",
    "    y = df[['next_x', 'next_y', 'next_z', 'next_max_wind_kn', 'next_min_pressure_mBar', \n",
    "            'next_velocity_kn', 'next_direction_sin', 'next_direction_cos']]\n",
    "\n",
    "    # Use the specified splitter\n",
    "    if splitter == 'gkf':\n",
    "        splitter = GroupKFold(n_splits=n_splits)\n",
    "    elif splitter == 'gss':\n",
    "        splitter = GroupShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid splitter type. Use 'gkf' for GroupKFold or 'gss' for GroupShuffleSplit.\")\n",
    "\n",
    "    groups = df['id']\n",
    "    \n",
    "    # Perform the split and yield train/test sets\n",
    "    for train_idx, test_idx in splitter.split(X, y, groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        yield X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2fdb0a-fa74-498d-9fab-a50598208de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nhc[['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', 'velocity_kn', \n",
    "        'direction_sin', 'direction_cos', 'enso']]\n",
    "\n",
    "y = nhc[['next_x', 'next_y', 'next_z', 'next_max_wind_kn', 'next_min_pressure_mBar', \n",
    "        'next_velocity_kn', 'next_direction_sin', 'next_direction_cos']]\n",
    "\n",
    "groups = nhc['id']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=97)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "# Step 5: Split the dataset\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4fba4-2130-436d-8670-45100f6f0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "        ('scaler', MinMaxScaler(), ['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', \n",
    "                                    'velocity_kn', 'direction_sin', 'direction_cos'])\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7f12d-53e1-4938-a593-53a73eaa5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=97)))\n",
    "    ])\n",
    "\n",
    "# Step 6: Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict and evaluate\n",
    "score = model.score(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5567d-e24d-4152-bb20-6afc760c27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ef1a3-edc4-44a5-9638-9171d4a1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac9e1-1c50-4aba-a418-4edbd9497ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c4273-f6f3-4b71-bee6-0ac4101ae4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7592e8-487a-43c9-9cb5-86377f11ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8559c-ae7c-4fc1-9ffa-2efabe5e0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1926b-4d8f-45b4-948e-2c4659ad2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_feature_names_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df991fa3-0851-450d-a97a-aeba32f9149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(SGDRegressor(max_iter=5000, tol=1e-3, random_state=97)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cdbe8-7d4c-4ceb-ac8c-41bd55de5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2717d-c5c5-4ea3-aaa7-08cd302dcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bc084-3997-4407-9cc7-0a60da2ee133",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__estimator__alpha': [0.0001, 0.001, 0.01],\n",
    "    'regressor__estimator__max_iter': [3000, 5000, 7000],\n",
    "    'regressor__estimator__penalty': ['l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline_sgd, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253d6f3-1b85-4a88-9561-910ff0a36bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline_sgd = pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(SGDRegressor(max_iter=3000, penalty='l2', alpha=1e-4, tol=1e-3, random_state=97)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f766c8-ea05-448a-9a79-cba8a514f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86de26-b373-4801-9bce-d4a0eed6328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a659fa-be9e-42c4-a32b-32fa3cb07d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target names (ensure these match the columns you used as targets in the training)\n",
    "target_names = ['next_x', 'next_y', 'next_z', 'next_max_wind_kn', 'next_min_pressure_mBar', 'next_velocity_kn', 'next_direction_sin', 'next_direction_cos']\n",
    "\n",
    "# Access the regressor inside the MultiOutputRegressor\n",
    "multi_output_regressor = best_pipeline_sgd.named_steps['regressor']\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_pipeline_sgd.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Loop through each target's regressor (each SGDRegressor model) with target names\n",
    "for i, (estimator, target_name) in enumerate(zip(multi_output_regressor.estimators_, target_names)):\n",
    "    print(f\"Target: {target_name}\")\n",
    "    coefficients = estimator.coef_\n",
    "    for feature, coef in zip(feature_names, coefficients):\n",
    "        print(f\"  Feature: {feature}, Coefficient: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627acb56-60fb-45b2-add2-9bd67879d283",
   "metadata": {},
   "source": [
    "Interpreting the Coefficients\n",
    "Positive Coefficients: If a feature has a positive coefficient, it means that as the value of this feature increases, the target value (prediction) increases, holding other features constant.\n",
    "Negative Coefficients: If a feature has a negative coefficient, it means that as the value of this feature increases, the target value decreases.\n",
    "Magnitude: The larger the absolute value of the coefficient, the more important the feature is. Features with very small coefficients (close to zero) have little influence on the model's predictions.\n",
    "\n",
    "This output shows the coefficients of each feature for your SGDRegressor model (Stochastic Gradient Descent) in a MultiOutputRegressor setup, where the model is predicting multiple targets (like next_x, next_y, etc.) simultaneously. Here’s a breakdown of what each part means:\n",
    "\n",
    "General Structure\n",
    "Target: For each target variable (e.g., next_x, next_y, next_z), the model has its own set of coefficients.\n",
    "Feature: These are the input features (e.g., enso, x, y, z, max_wind_kn, etc.) that are used to predict the next timestep's values.\n",
    "Coefficient: This is the weight (importance) assigned to each feature by the SGDRegressor for predicting a specific target.\n",
    "Explanation of Key Components\n",
    "Coefficients for Each Target:\n",
    "\n",
    "For each target (e.g., next_x, next_y), the model assigns a different weight (coefficient) to each feature.\n",
    "The features include the ENSO phase (enso__enso_-1, enso__enso_0, enso__enso_1), as well as scaled features like x, y, z, and others.\n",
    "ENSO Phases:\n",
    "\n",
    "The ENSO phase (El Niño, La Niña, Neutral) has a small effect in most cases, as the coefficients associated with enso__enso_-1, enso__enso_0, and enso__enso_1 are generally quite small. This suggests that while the ENSO phase does contribute to the model, it is not a dominant factor for predicting future TD positions or characteristics.\n",
    "Spatial Features (x, y, z):\n",
    "\n",
    "For predicting the next position (next_x, next_y, next_z), the spatial features like x, y, and z have higher coefficients. For example:\n",
    "In next_x, scaler__x has a coefficient of 0.980, indicating that the current x position plays a strong role in predicting the next x position.\n",
    "Similarly, for next_y, the scaler__y coefficient is 1.234, showing that the y coordinate is heavily influencing the prediction of next_y.\n",
    "These high coefficients make sense because the model relies on the current position to predict the next one.\n",
    "Wind and Pressure:\n",
    "\n",
    "For targets like next_max_wind_kn and next_min_pressure_mBar, the model assigns significant coefficients to wind speed and pressure:\n",
    "For next_max_wind_kn, scaler__max_wind_kn has a very high coefficient (141.79), indicating that current wind speed is a key factor in predicting future wind speed.\n",
    "Similarly, next_min_pressure_mBar has a high coefficient for scaler__min_pressure_mBar, showing the importance of current pressure in predicting future pressure.\n",
    "Direction (direction_sin and direction_cos):\n",
    "\n",
    "The features direction_sin and direction_cos have smaller coefficients, indicating that while they contribute to the prediction, they are not as influential as the spatial or wind-related features.\n",
    "Key Takeaways:\n",
    "Dominant Features: Spatial features (x, y, z) and wind/pressure play the most important role in predicting future tropical depression coordinates and characteristics.\n",
    "ENSO Impact: The impact of ENSO phases on the predictions is minor, as indicated by the small coefficients.\n",
    "Multi-Output Setup: Each target (e.g., next_x, next_y, next_z) has its own set of coefficients, meaning the model independently adjusts its behavior for each prediction task.\n",
    "This analysis helps explain how the model is using the available features to predict future values for tropical depression position and intensity, with clear emphasis on certain physical characteristics like position and wind speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5b213-dcea-412a-9ed5-2e7f675d87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_poly = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "        ('scaler', MinMaxScaler(), ['max_wind_kn', 'min_pressure_mBar', 'velocity_kn']),\n",
    "        ('poly', PolynomialFeatures(degree=1), ['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', 'velocity_kn', \n",
    "        'direction_sin', 'direction_cos']),\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4e04f-99c4-4ffa-bbc2-8af8fb403a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_poly_sgd = Pipeline([\n",
    "    ('preprocess', preprocessor_poly),           # Preprocess (scaling, encoding)\n",
    "    ('regressor', MultiOutputRegressor(SGDRegressor(max_iter=3000, penalty='l2', alpha=1e-4, tol=1e-3, random_state=97)))  # Train the model\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fdd60-5397-4cc1-930c-a6adcc922eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_poly_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe00ce-c090-4b69-bf4a-3a2f22108b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_poly_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf58795-8274-4f5c-8c54-98d1e75e4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_poly_sgd.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5fd29-64de-4437-92e0-2c1b50da842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f28bd6-5fba-45d2-899d-8205e2b1d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc65b0-34a1-4c89-a333-a57f52f6c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "        ('scaler', MinMaxScaler(), ['max_wind_kn', 'min_pressure_mBar', 'velocity_kn']),\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8df63-cac4-4733-a2f1-0f3f0b4764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(SGDRegressor(max_iter=3000, penalty='l2', alpha=1e-4, tol=1e-3, random_state=97)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306cf27-c2f4-424d-b077-398b993c4793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__estimator__alpha': [1e-6, 1e-5, 1e-4],\n",
    "    'regressor__estimator__max_iter': [3000, 5000, 7000],\n",
    "    'regressor__estimator__learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
    "    'regressor__estimator__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "}\n",
    "\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline_sgd, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6f83f-0592-422b-bdeb-0426a811acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc90b93-f4c4-4d0f-a642-8bd5010085d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbb0f7-10ec-4d0d-ad8a-a917bdeed827",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(SGDRegressor(learning_rate='adaptive', max_iter=5000, penalty='l1', alpha=1e-4, tol=1e-3, random_state=97)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4a5fc-702f-4e1c-b577-eb600e05ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45901e38-65a8-4055-aaf9-726aad3f8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33c7fe-9da4-4d20-ac6a-ef528e8f1e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__estimator__learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
    "              }\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline_sgd, param_grid, cv=None, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7f92a-5c82-4324-acd8-8ea83c960282",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MultiOutputRegressor(SGDRegressor(learning_rate='adaptive', max_iter=3000, penalty='l1', alpha=1e-6, tol=1e-5, random_state=97)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31298b82-4499-40f6-b6d6-ac2e13682b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f2ed8-0091-4891-ba6d-f9f632311ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcbbde-149c-43fb-94d3-a98b0c842286",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01769532-c7fb-446d-93d7-ee370f9039fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = next(split_grouped_dataframe(jma, splitter='gss', n_splits=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8841fa0-3901-4830-8619-baf5dc6dd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_final', index_col=0)\n",
    "for X_train, X_test, y_train, y_test in split_grouped_dataframe(jma, splitter='gss', n_splits=2):\n",
    "    pipeline_sgd.fit(X_train, y_train)\n",
    "    print(pipeline_sgd.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb935da-002c-4609-b3e0-997de912fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc = pd.read_csv('data/csv_ready/nhc_final', index_col=0)\n",
    "for X_train, X_test, y_train, y_test in split_grouped_dataframe(nhc, splitter='gss', n_splits=1):\n",
    "    partial_fit(X_train, y_train, pipeline_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24df5b-b987-48ed-aee3-a031d652c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_jma, X_test_jma, y_train_jma, y_test_jma = next(split_grouped_dataframe(jma, splitter='gss', n_splits=1))\n",
    "X_train_nhc, X_test_nhc, y_train_nhc, y_test_nhc = next(split_grouped_dataframe(nhc, splitter='gss', n_splits=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b9c2d-d3cf-4e3e-ad99-7e4317344fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd.fit(X_train_jma, y_train_jma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb3ac7-17a8-497f-a6d7-63d50194c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring the model on the test data of the same ocean region\n",
    "pipeline_sgd.score(X_test_jma, y_test_jma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212985d-b894-40d0-b3d5-4837f15d77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring the model on data of NE/Central pacific region before trainig\n",
    "pipeline_sgd.score(X_test_nhc, y_test_nhc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3f616-fad5-4f14-b27c-a375aa9c5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionallly train the model for the NE Pacific\n",
    "partial_fit(X_train_nhc, y_train_nhc, pipeline_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec0fdf-552f-413d-82fe-20c0cdea5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model on NE Pacific\n",
    "pipeline_sgd.score(X_test_nhc, y_test_nhc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520faf26-bee2-4172-846b-50b6db220590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see if the model for NW Pacific now will score higher or lower now that it knows data from NE PAcific\n",
    "pipeline_sgd.score(X_test_jma, y_test_jma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0881c-0da6-4a41-92c9-a7f3a9741d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(X, y, pipeline, preprocess='preprocessor', regressor='regressor'):\n",
    "   \n",
    "    X = pipeline.named_steps[preprocess].transform(X)\n",
    "\n",
    "    multi_output_regressor = pipeline.named_steps[regressor]\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.values\n",
    "        \n",
    "    for i, estimator in enumerate(multi_output_regressor.estimators_):\n",
    "        # partial_fit for the i-th target\n",
    "        estimator.partial_fit(X, y[:, i])\n",
    "\n",
    "    print(\"Pipeline partially fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2357b-e651-40fb-8ae7-ce13c3a55ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial_fit(X_test, y_test, pipeline_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c23f44-8435-448b-b9b5-bef13b7e7457",
   "metadata": {},
   "source": [
    "Now the pipline is fitted wit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688c2b3-a7f3-4e34-9e54-dc4e64c02a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc = pd.read_csv('data/csv_ready/nhc_training', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a390b-5782-413b-ae3c-6ba2a23b7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = FunctionTransformer(func=prepare_dataframe)\n",
    "prep_train_data = FunctionTransformer(func=generate_training_dataframe)\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "        ('scaler', MinMaxScaler(), ['max_wind_kn', 'min_pressure_mBar', 'velocity_kn']),\n",
    "    ], remainder='passthrough')\n",
    "regressor = MultiOutputRegressor(SGDRegressor(learning_rate='adaptive', max_iter=3000, penalty='l1', alpha=1e-6, tol=1e-5, random_state=97))\n",
    "\n",
    "nhc_pred_pipeline_sgd = Pipeline([\n",
    "        ('prep_data', prep_data),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "nhc_train_pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b092bb-9257-4625-8678-dee64fdecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = prep_data.transform(nhc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d97d48-1d6f-4fa3-8448-bcf901daeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = prep_train_data.transform(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295d831-06b7-43ba-89e7-2e2cfb287347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, splitter='gss', n_splits=1, test_size=0.2, random_state=97):\n",
    "     # Define the features and target columns\n",
    "    X = df[['x', 'y', 'z', 'max_wind_kn', 'min_pressure_mBar', 'velocity_kn', \n",
    "            'direction_sin', 'direction_cos', 'enso']]\n",
    "    y = df[['next_x', 'next_y', 'next_z', 'next_max_wind_kn', 'next_min_pressure_mBar', \n",
    "            'next_velocity_kn', 'next_direction_sin', 'next_direction_cos']]\n",
    "\n",
    "    # Use the specified splitter\n",
    "    if splitter == 'gkf':\n",
    "        splitter = GroupKFold(n_splits=n_splits)\n",
    "    elif splitter == 'gss':\n",
    "        splitter = GroupShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid splitter type. Use 'gkf' for GroupKFold or 'gss' for GroupShuffleSplit.\")\n",
    "\n",
    "    groups = df['group']\n",
    "    \n",
    "    # Perform the split and yield train/test sets\n",
    "    for train_idx, test_idx in splitter.split(X, y, groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        yield X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739d861-b833-4a90-9b8e-a9cd5c7c06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = next(split_dataframe(df_trn, splitter='gss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdc5b0-049f-4596-a55c-2521a053f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc_train_pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2930bce-6c55-4844-9785-02a208cc6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc_train_pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379df7f-775a-4f85-a0e0-58d9923dc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc[43:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3d2b0-f072-417e-9a9c-e2f2f7dbff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = nhc[43:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebf8ac-66c3-4fbe-8601-862a1fd5da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855ac01-dfd1-430a-bdb6-6a058bf1cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manage_prediction(X_pred, nhc_pred_pipeline_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d044c4e-4bc6-438e-b296-a96741d1dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc_pred_pipeline_sgd.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ede2b-cc53-48e6-a8cd-51a190c93277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manage_prediction(df, model):\n",
    "    predicted_values = model.predict(df)\n",
    "    x, y, z, max_wind_kn, min_pressure_mBar, velocity_kn, direction_sin, direction_cos = predicted_values[0]\n",
    "    \n",
    "    lat, lon = cartesian_to_geo(x, y, z)\n",
    "    min_pressure_mBar = int(min_pressure_mBar)\n",
    "    max_wind_kn = np.round(max_wind_kn, 1)\n",
    "    velocity_kn = np.round(velocity_kn, 1)\n",
    "    direction_deg = get_direction_from_sin_cos(direction_sin, direction_cos)\n",
    "\n",
    "    return lat, lon, max_wind_kn, min_pressure_mBar, velocity_kn, direction_deg\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac8575-c0ff-4b0d-9d42-ef7d7056ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc[44:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c1ad4-a89a-4f9f-9230-d2c2d6ae29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc_pred_pipeline_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68301dea-2e79-45bb-8b57-4b4e45b3755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = nhc_pred_pipeline_sgd.named_steps['preprocessor'].named_transformers_['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c6b4d-2235-467d-b4a1-760ba9c8bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nhc_pred_pipeline_sgd.named_steps['prep_data'].transform(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07800a-2037-4dc6-9079-72cdbe1ddbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc_pred_pipeline_sgd.named_steps['preprocessor'].transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64798aa5-c838-4043-aef6-4b439f7fe19a",
   "metadata": {},
   "source": [
    "The dataframe is not correct. Should lag the speed and dir by (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e9423-0aca-47bf-aabc-d2deb59d3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ce519-1c82-4588-b716-2f98ae498c15",
   "metadata": {},
   "source": [
    "Random forest takes 4 hours to be trained. And is of absolutely no use!!! So wrong...\n",
    "Will try to train the model to predict only coordinates, without direction and speed, will user XGBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec7a29-3ccb-4d60-82e0-e02da5cf0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_training', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b21d58-ac5d-467a-bf82-b83a7a3f02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = FunctionTransformer(func=prepare_dataframe)\n",
    "prep_train_data = FunctionTransformer(func=generate_training_dataframe)\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),\n",
    "    ], remainder='passthrough', force_int_remainder_cols=False)\n",
    "scaler = MinMaxScaler()\n",
    "regressor = MultiOutputRegressor(SGDRegressor(learning_rate='adaptive', max_iter=5000, penalty='l1', alpha=1e-6, tol=1e-5, random_state=97))\n",
    "\n",
    "jma_pred_pipeline_sgd = Pipeline([\n",
    "        ('prep_data', prep_data),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "jma_train_pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a18b05-a4a5-4ac6-b73c-68d9900f2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = prep_data.transform(jma)\n",
    "# df_trn = prep_train_data.transform(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49edf733-8bac-41cf-84a4-4e4ecda38a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn.to_csv('data/csv_ready/df_trn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698123b-9196-43ce-9cfe-019a066ecc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv('data/csv_ready/df_trn.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7b869-b232-42d5-a53e-304b31393a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = next(split_dataframe(df_trn, splitter='gss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2667c-63e8-4547-96e0-4f079e24999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma_train_pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071e008-e0dd-4982-968a-d811c5edba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "jma_train_pipeline_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff814a6-a30f-46e7-a342-2a7c80c28f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = jma[jma.group == 1031]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da5338-6be8-440d-b436-5c6dadae32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred[df_pred.group == 1031].copy()\n",
    "\n",
    "X_pred = df_pred[1:2]\n",
    "X_expected = df_pred[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd974c-5480-437d-a728-2de599eefaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a866d-6762-4419-b46c-5349c1786f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451aa7f-baac-489a-8048-8a5f9ecfd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "manage_prediction(X_pred, jma_pred_pipeline_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e1acb-4c4e-47bd-923f-db6dcfdc2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tds(jma, X_test, model=jma_pred_pipeline_sgd, n_samples=3, random_state=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e92325-b364-4a1a-801b-6920f1219600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = create_real_pred_df(jma, X_test, model=jma_pred_pipeline_sgd, n_samples=3, random_state=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088531a4-3a7c-46cc-8782-2268ed7e72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = probability_within_radius(df_, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49a98a-fe7f-47a2-b7b9-dc84241b615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.probability_within_radius.min(), df_.probability_within_radius.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d248ecd-e2a0-4adb-a1be-564a685de52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[df_.group == 609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe70ec6-f790-4a55-afb3-4c2e9f666906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
